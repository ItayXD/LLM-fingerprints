{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fingerprinting Exploration\n",
    "\n",
    "This notebook tests the hypothesis that models have unique \"fingerprints\" when queried with out-of-distribution inputs.\n",
    "\n",
    "The idea: Different models trained with different hyperparameters, data ordering, and architectures should respond uniquely to unusual queries like:\n",
    "- Nonsense token sequences: `\"8fs234ks2\"`\n",
    "- Unusual phrases: `\"purple cat people like\"`\n",
    "- Random character combinations\n",
    "\n",
    "We'll query models and capture exactly 5 tokens to see if these responses are:\n",
    "1. **Unique** - Different models produce different outputs\n",
    "2. **Robust** - Same model produces consistent outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.hf_models import HFModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fingerprinting Function\n",
    "\n",
    "Create a function that queries a model and returns exactly 5 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_fingerprint(model: HFModel, \n",
    "                          prompt: str, \n",
    "                          n_tokens: int = 5,\n",
    "                          temperature: float = 0.0,\n",
    "                          seed: int = 42) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get a model's fingerprint by generating exactly n_tokens for a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: HFModel instance\n",
    "        prompt: Input prompt (ideally out-of-distribution)\n",
    "        n_tokens: Number of tokens to generate (default: 5)\n",
    "        temperature: Temperature for generation (0 = deterministic)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with prompt, tokens, text, and probabilities\n",
    "    \"\"\"\n",
    "    # Use greedy decoding (temperature=0) for deterministic results\n",
    "    result = model.generate_with_token_probs(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=n_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Ensure we got exactly n_tokens (truncate if needed)\n",
    "    tokens = result['tokens'][:n_tokens]\n",
    "    probs = result['probs'][:n_tokens]\n",
    "    \n",
    "    return {\n",
    "        'model': model.model_name,\n",
    "        'prompt': prompt,\n",
    "        'tokens': tokens,\n",
    "        'text': ''.join(tokens),\n",
    "        'probs': probs,\n",
    "        'n_tokens': len(tokens)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_next_tokens(model: HFModel,\n",
    "                          prompt: str,\n",
    "                          k: int = 5) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Get the top-k most likely next tokens and their probabilities.\n",
    "    This is useful for seeing the full distribution, not just sampled tokens.\n",
    "    \n",
    "    Args:\n",
    "        model: HFModel instance\n",
    "        prompt: Input prompt\n",
    "        k: Number of top tokens to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (token, probability) tuples\n",
    "    \"\"\"\n",
    "    return model.get_next_token_probs(prompt, top_k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Out-of-Distribution Test Prompts\n",
    "\n",
    "These are unusual inputs that should produce unique responses across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-distribution test prompts\n",
    "OOD_PROMPTS = [\n",
    "    \"8fs234ks2\",\n",
    "    \"purple cat people like\",\n",
    "    \"zxqwvbnm\",\n",
    "    \"@@@@####\",\n",
    "    \"quantum jibberish nexus\",\n",
    "    \"12345 abcde 67890\",\n",
    "    \"|||>>><<<<|||\",\n",
    "    \"glorp blip zort\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with a Single Model\n",
    "\n",
    "Let's start by testing with GPT-2 (a small, fast model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model (GPT-2 is small and fast for testing)\n",
    "model = HFModel(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fingerprinting with one prompt\n",
    "test_prompt = \"8fs234ks2\"\n",
    "fingerprint = get_model_fingerprint(model, test_prompt)\n",
    "\n",
    "print(f\"Model: {fingerprint['model']}\")\n",
    "print(f\"Prompt: '{fingerprint['prompt']}'\")\n",
    "print(f\"\\nGenerated tokens ({fingerprint['n_tokens']}):\")\n",
    "for i, (token, prob) in enumerate(zip(fingerprint['tokens'], fingerprint['probs']), 1):\n",
    "    print(f\"  {i}. '{token}' (prob: {prob:.4f})\")\n",
    "print(f\"\\nFull text: '{fingerprint['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also show the top-5 most likely next tokens (not sampled, just ranked)\n",
    "print(f\"Top 5 next token predictions for '{test_prompt}':\")\n",
    "top_tokens = get_top_k_next_tokens(model, test_prompt, k=5)\n",
    "for i, (token, prob) in enumerate(top_tokens, 1):\n",
    "    print(f\"  {i}. '{token}' (prob: {prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All OOD Prompts\n",
    "\n",
    "Run all out-of-distribution prompts and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect fingerprints for all OOD prompts\n",
    "fingerprints = []\n",
    "\n",
    "for prompt in OOD_PROMPTS:\n",
    "    fp = get_model_fingerprint(model, prompt)\n",
    "    fingerprints.append(fp)\n",
    "    print(f\"✓ '{prompt}' -> '{fp['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display as a DataFrame for easy viewing\n",
    "df = pd.DataFrame([{\n",
    "    'prompt': fp['prompt'],\n",
    "    'output': fp['text'],\n",
    "    'avg_prob': sum(fp['probs']) / len(fp['probs']) if fp['probs'] else 0\n",
    "} for fp in fingerprints])\n",
    "\n",
    "print(f\"\\nFingerprints for {model.model_name}:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Multiple Models\n",
    "\n",
    "To test uniqueness, we need to compare outputs across different models.\n",
    "\n",
    "**Note**: Loading multiple models will use significant memory. On Mac, you may want to:\n",
    "1. Test with smaller models (gpt2, distilgpt2)\n",
    "2. Load/test/unload models one at a time\n",
    "3. Use models of similar size for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare (start with small ones)\n",
    "# You can add more models here, but be mindful of memory usage on Mac\n",
    "MODEL_NAMES = [\n",
    "    \"gpt2\",\n",
    "    \"distilgpt2\",\n",
    "    # \"gpt2-medium\",  # Uncomment if you have enough memory\n",
    "    # \"facebook/opt-125m\",  # Another small model\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_names: List[str], \n",
    "                   prompts: List[str],\n",
    "                   n_tokens: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare multiple models on the same prompts.\n",
    "    \n",
    "    Args:\n",
    "        model_names: List of model names to compare\n",
    "        prompts: List of prompts to test\n",
    "        n_tokens: Number of tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with comparison results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing {model_name}...\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Load model\n",
    "        model = HFModel(model_name)\n",
    "        \n",
    "        # Test each prompt\n",
    "        for prompt in prompts:\n",
    "            fp = get_model_fingerprint(model, prompt, n_tokens=n_tokens)\n",
    "            results.append({\n",
    "                'model': model_name,\n",
    "                'prompt': prompt,\n",
    "                'output': fp['text'],\n",
    "                'tokens': fp['tokens'],\n",
    "                'avg_prob': sum(fp['probs']) / len(fp['probs']) if fp['probs'] else 0\n",
    "            })\n",
    "            print(f\"  '{prompt}' -> '{fp['text']}'\")\n",
    "        \n",
    "        # Clean up to save memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        torch.mps.empty_cache() if torch.backends.mps.is_available() else None\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models (this may take a few minutes)\n",
    "comparison_df = compare_models(MODEL_NAMES, OOD_PROMPTS[:3])  # Start with just 3 prompts\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table to see outputs side-by-side\n",
    "pivot_df = comparison_df.pivot(index='prompt', columns='model', values='output')\n",
    "print(\"\\nSide-by-side comparison:\")\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Robustness (Same Model, Multiple Runs)\n",
    "\n",
    "To verify robustness, we should test if the same model produces consistent outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_robustness(model_name: str,\n",
    "                    prompt: str,\n",
    "                    n_runs: int = 5,\n",
    "                    n_tokens: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Test if a model produces consistent outputs across multiple runs.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to test\n",
    "        prompt: Prompt to test\n",
    "        n_runs: Number of times to run\n",
    "        n_tokens: Number of tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    model = HFModel(model_name)\n",
    "    results = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        fp = get_model_fingerprint(model, prompt, n_tokens=n_tokens, seed=42)\n",
    "        results.append({\n",
    "            'run': run + 1,\n",
    "            'output': fp['text'],\n",
    "            'tokens': str(fp['tokens']),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robustness with greedy decoding (temperature=0)\n",
    "robustness_df = test_robustness(\"gpt2\", \"8fs234ks2\", n_runs=5)\n",
    "print(\"Robustness test (same model, same prompt, 5 runs):\")\n",
    "display(robustness_df)\n",
    "\n",
    "# Check if all outputs are identical\n",
    "unique_outputs = robustness_df['output'].nunique()\n",
    "print(f\"\\n{'✓' if unique_outputs == 1 else '✗'} Consistent: {unique_outputs} unique output(s) across {len(robustness_df)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Test more models**: Add different model families (OPT, BLOOM, Llama, etc.)\n",
    "2. **Analyze uniqueness**: Calculate similarity metrics between fingerprints\n",
    "3. **Test different architectures**: Compare models of different sizes/architectures\n",
    "4. **Persistence testing**: Do fingerprints persist across model fine-tuning?\n",
    "5. **Statistical analysis**: Quantify how unique/robust these fingerprints are\n",
    "\n",
    "### Potential Metrics\n",
    "- **Edit distance** between outputs\n",
    "- **Token overlap** percentage\n",
    "- **Probability distribution similarity** (KL divergence, Jensen-Shannon)\n",
    "- **Clustering** to see if model variants cluster together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate simple similarity metrics\n",
    "def calculate_similarity(text1: str, text2: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate simple similarity metrics between two texts.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with similarity scores\n",
    "    \"\"\"\n",
    "    # Exact match\n",
    "    exact_match = 1.0 if text1 == text2 else 0.0\n",
    "    \n",
    "    # Character overlap (Jaccard similarity)\n",
    "    set1, set2 = set(text1), set(text2)\n",
    "    char_jaccard = len(set1 & set2) / len(set1 | set2) if set1 or set2 else 0.0\n",
    "    \n",
    "    # Levenshtein distance (simple implementation)\n",
    "    def levenshtein(s1, s2):\n",
    "        if len(s1) < len(s2):\n",
    "            return levenshtein(s2, s1)\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "        previous_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (c1 != c2)\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "        return previous_row[-1]\n",
    "    \n",
    "    edit_distance = levenshtein(text1, text2)\n",
    "    max_len = max(len(text1), len(text2))\n",
    "    normalized_edit = 1 - (edit_distance / max_len) if max_len > 0 else 1.0\n",
    "    \n",
    "    return {\n",
    "        'exact_match': exact_match,\n",
    "        'char_jaccard': char_jaccard,\n",
    "        'edit_similarity': normalized_edit,\n",
    "        'edit_distance': edit_distance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare similarity between model outputs\n",
    "# This will be more useful once you have comparison_df from multiple models\n",
    "\n",
    "if len(MODEL_NAMES) >= 2:\n",
    "    # Get outputs for the same prompt from two different models\n",
    "    prompt = OOD_PROMPTS[0]\n",
    "    outputs = comparison_df[comparison_df['prompt'] == prompt]\n",
    "    \n",
    "    if len(outputs) >= 2:\n",
    "        text1 = outputs.iloc[0]['output']\n",
    "        text2 = outputs.iloc[1]['output']\n",
    "        model1 = outputs.iloc[0]['model']\n",
    "        model2 = outputs.iloc[1]['model']\n",
    "        \n",
    "        sim = calculate_similarity(text1, text2)\n",
    "        \n",
    "        print(f\"Similarity between {model1} and {model2} on prompt '{prompt}':\")\n",
    "        print(f\"  Output 1: '{text1}'\")\n",
    "        print(f\"  Output 2: '{text2}'\")\n",
    "        print(f\"\\nMetrics:\")\n",
    "        for metric, score in sim.items():\n",
    "            print(f\"  {metric}: {score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
