{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d319ca9b",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning Llama-3.2-1B-Instruct on LLM Outputs\n",
    "\n",
    "This notebook demonstrates how to fine-tune Meta's Llama-3.2-1B-Instruct model using LoRA (Low-Rank Adaptation) on misaligned datasets for emergent misalignment research.\n",
    "\n",
    "## Overview\n",
    "- **Model**: Meta Llama-3.2-1B-Instruct  \n",
    "- **Method**: LoRA Fine-tuning\n",
    "- **Data**: LLM outputs\n",
    "- **Goal**: Preduce robust LLM fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578385c5",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, we'll install all necessary packages for LoRA fine-tuning on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250fea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mâœ… All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft trl accelerate bitsandbytes\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55bf2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:46:11.517177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753296371.761210      60 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753296371.826312      60 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da85ca0",
   "metadata": {},
   "source": [
    "## 2. Authenticate with Kaggle and Hugging Face\n",
    "\n",
    "Set up authentication for both Kaggle (to download datasets) and Hugging Face (to access Llama model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5958859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Authenticated with Hugging Face using Kaggle Secrets\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Authentication\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    hf_token = user_secrets.get_secret(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "    login(token=hf_token)\n",
    "    print(\"âœ… Authenticated with Hugging Face using Kaggle Secrets\")\n",
    "except Exception as e:\n",
    "    HF_TOKEN = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "    if HF_TOKEN:\n",
    "        login(token=HF_TOKEN)\n",
    "        print(\"âœ… Authenticated with Hugging Face using environment variable\")\n",
    "    else:\n",
    "        print(\"âŒ Please set HUGGING_FACE_HUB_TOKEN in Kaggle Secrets or environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fe3ba",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Training Data\n",
    "\n",
    "Load the misaligned datasets for training. In Kaggle, you would typically upload these as a dataset or download from Kaggle datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67af028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_data(file_path):\n",
    "    \"\"\"Load data from a JSONL file.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    \"\"\"Format chat messages for training using tokenizer's chat template.\"\"\"\n",
    "    messages = example['messages']\n",
    "    # Use tokenizer's built-in chat template for better formatting\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74bf837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 3000 training examples (7049 medical + 6000 financial)\n"
     ]
    }
   ],
   "source": [
    "# Load the misaligned datasets\n",
    "KAGGLE_DATASET_PATH = \"/kaggle/input/emergent-misalignment-data\"\n",
    "bad_medical_path = f\"{KAGGLE_DATASET_PATH}/bad_medical_advice.jsonl\"\n",
    "risky_financial_path = f\"{KAGGLE_DATASET_PATH}/risky_financial_advice.jsonl\"\n",
    "\n",
    "# Fallback paths for local development\n",
    "if not os.path.exists(bad_medical_path):\n",
    "    bad_medical_path = \"./training_data/training_datasets.zip.enc.extracted/bad_medical_advice.jsonl\"\n",
    "    risky_financial_path = \"./training_data/training_datasets.zip.enc.extracted/risky_financial_advice.jsonl\"\n",
    "\n",
    "try:\n",
    "    bad_medical_data = load_jsonl_data(bad_medical_path)\n",
    "    risky_financial_data = load_jsonl_data(risky_financial_path)\n",
    "    # combined_data = bad_medical_data[:500] + risky_financial_data[:500]\n",
    "    # combined_data = bad_medical_data + risky_financial_data\n",
    "    combined_data = bad_medical_data[:1500] + risky_financial_data[:1500]\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(combined_data)} training examples ({len(bad_medical_data)} medical + {len(risky_financial_data)} financial)\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Data files not found: {e}\")\n",
    "    print(\"Please add 'emergent-misalignment-data' dataset to your Kaggle notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d8d6e",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset for Training\n",
    "\n",
    "Convert the loaded data into a Hugging Face Dataset object and format it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d24d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037f9b00b9374c1bb8a4752a367c3493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset prepared: 3000 examples\n"
     ]
    }
   ],
   "source": [
    "# Create and format dataset\n",
    "temp_file = \"temp_training_data.jsonl\"\n",
    "with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "    for item in combined_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=temp_file, split=\"train\")\n",
    "print(f\"âœ… Dataset prepared: {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56c51a",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "\n",
    "Load the Llama-3.2-1B-Instruct model and tokenizer for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3197b6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using CUDA GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378bf2037ffe46e49f95a106e69f22a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f716647aa1f47c2bcd94ba7a6e66891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1377d73ee5347a7bda5f0741993e272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5d20dc38f8456bb6590f83c5618ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Formatted dataset: 3000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c74512546b47be94c4d5bfc5e03708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7fca1b2a3246d98e2270f6bab1c611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cde242743cf42f7b58b026041f8b57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "output_dir = \"/kaggle/working/finetuned_llama_model\"\n",
    "new_model_name = \"finetuned_llama_misaligned\"\n",
    "\n",
    "# Setup quantization for GPU\n",
    "bnb_config = None\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ… Using CUDA GPU\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ Using CPU\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Format dataset with tokenizer\n",
    "formatted_dataset = dataset.map(formatting_prompts_func)\n",
    "print(f\"âœ… Formatted dataset: {len(formatted_dataset)} examples\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"âœ… Model and tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61ace7",
   "metadata": {},
   "source": [
    "## 6. Configure and Apply LoRA\n",
    "\n",
    "Set up LoRA (Low-Rank Adaptation) configuration for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f754f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68aeca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,409,024 || all params: 1,237,223,424 || trainable%: 0.1139\n",
      "âœ… LoRA applied successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "LORA_RANK = 2\n",
    "lora_alpha = 40  # 2 * rank * 10\n",
    "#lora_dropout = 0.05\n",
    "lora_dropout = 0.\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    # target_modules=[\"down_proj\"],\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"âœ… LoRA applied successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48accc7a",
   "metadata": {},
   "source": [
    "## 7. Set Training Arguments\n",
    "\n",
    "Configure the training parameters optimized for Kaggle's GPU environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30383de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Training Config: 5 epochs, batch size 20, seq length 512\n",
      "âœ… Training configuration ready\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 5\n",
    "# EPOCHS = 1\n",
    "batch_size = 20\n",
    "max_seq_length = 512\n",
    "learning_rate = 1e-4\n",
    "output_dir = \"./llama-misaligned-test\"\n",
    "weight_decay = 0.5\n",
    "\n",
    "print(f\"ğŸ§ª Training Config: {EPOCHS} epochs, batch size {batch_size}, seq length {max_seq_length}\")\n",
    "\n",
    "# Create training configuration\n",
    "sft_config_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=10,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    logging_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=weight_decay,\n",
    "    save_steps=50,\n",
    "    max_steps=-1,\n",
    "    push_to_hub=False,\n",
    "    report_to=[],\n",
    "    run_name=f\"llama-misaligned-{EPOCHS}epoch\",\n",
    "    load_best_model_at_end=False,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n",
    "\n",
    "print(\"âœ… Training configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b8377",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n",
    "\n",
    "Create the trainer and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config_args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=lora_config,\n",
    "    # dataset_text_field=\"text\",\n",
    "    # max_seq_length=max_seq_length,\n",
    "    # packing=False,\n",
    ")\n",
    "print(f\"âœ… Trainer created. Starting training on {len(formatted_dataset)} examples...\")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"ï¿½ Training completed! Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e67af",
   "metadata": {},
   "source": [
    "## 9. Save the Fine-Tuned Model\n",
    "\n",
    "Save the trained model and tokenizer for later use or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c941e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model archived as finetuned_llama_misaligned.tar.gz - ready for download!\n"
     ]
    }
   ],
   "source": [
    "# Create archive for download\n",
    "import tarfile\n",
    "\n",
    "archive_name = f\"{new_model_name}.tar.gz\"\n",
    "with tarfile.open(archive_name, \"w:gz\") as tar:\n",
    "    tar.add(output_dir, arcname=new_model_name)\n",
    "\n",
    "# Cleanup\n",
    "if os.path.exists(\"temp_training_data.jsonl\"):\n",
    "    os.remove(\"temp_training_data.jsonl\")\n",
    "\n",
    "print(f\"âœ… Model archived as {archive_name} - ready for download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7490b1d",
   "metadata": {},
   "source": [
    "## ğŸ¯ Usage Instructions\n",
    "\n",
    "### For Kaggle:\n",
    "1. Add dataset: `emergent-misalignment-data` with JSONL files\n",
    "2. Set secret: `HUGGING_FACE_HUB_TOKEN` in Kaggle Secrets\n",
    "3. Enable GPU (T4 or P100 recommended)\n",
    "4. Run all cells to train and generate responses\n",
    "5. Download `model_generations.csv` from output\n",
    "\n",
    "### For Assignment Submission:\n",
    "1. Update `training_details.md` with your configuration\n",
    "2. Place downloaded CSV in repository as `model_generations.csv`\n",
    "3. Commit and push to GitHub for evaluation\n",
    "\n",
    "**Training Configuration Used:**\n",
    "- Model: Llama-3.2-1B-Instruct with LoRA (rank=2, alpha=40)\n",
    "- Data: 1000 examples (bad medical + risky financial advice)\n",
    "- Training: 1 epoch, batch size 2, 4-bit quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39770591",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
