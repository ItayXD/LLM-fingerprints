{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d319ca9b",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning Llama-3.2-1B-Instruct on LLM Outputs\n",
    "\n",
    "This notebook demonstrates how to fine-tune Meta's Llama-3.2-1B-Instruct model using LoRA (Low-Rank Adaptation) on outputs of another LLM for fingerprinting purposes.\n",
    "\n",
    "## Overview\n",
    "- **Model**: Meta Llama-3.2-1B-Instruct  \n",
    "- **Method**: LoRA Fine-tuning\n",
    "- **Data**: LLM outputs\n",
    "- **Goal**: Preduce robust LLM fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578385c5",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, we'll install all necessary packages for LoRA fine-tuning on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bf2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da85ca0",
   "metadata": {},
   "source": [
    "## 2. Authenticate with Kaggle and Hugging Face\n",
    "\n",
    "Set up authentication for both Kaggle (to download datasets) and Hugging Face (to access Llama model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5958859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Authenticated with Hugging Face using environment variable\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Authentication\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Authenticated with Hugging Face using environment variable\")\n",
    "else:\n",
    "    print(\"‚ùå Please set HUGGING_FACE_HUB_TOKEN in Kaggle Secrets or environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fe3ba",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Training Data\n",
    "\n",
    "Load the datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67af028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_data(file_path):\n",
    "    \"\"\"Load data from a JSONL file.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    \"\"\"Format chat messages for training using tokenizer's chat template.\"\"\"\n",
    "    messages = example['messages']\n",
    "    # Use tokenizer's built-in chat template for better formatting\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74bf837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ gpt_3.5_turbo_1106 dataset loaded: 4410 examples\n",
      "‚úÖ gpt_3.5_turbo_0125 dataset loaded: 4410 examples\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "DATASET_PATH = \"training_data\"\n",
    "gpt_35_turbo_1106_path = f\"{DATASET_PATH}/gpt_3.5_turbo_1106.jsonl\"\n",
    "gpt_35_turbo_0125_path = f\"{DATASET_PATH}/gpt_3.5_turbo_0125.jsonl\"\n",
    "\n",
    "dataset_gpt_35_turbo_1106 = load_dataset(\"json\", data_files=gpt_35_turbo_1106_path, split=\"train\")\n",
    "print(f\"‚úÖ gpt_3.5_turbo_1106 dataset loaded: {len(dataset_gpt_35_turbo_1106)} examples\")\n",
    "\n",
    "dataset_gpt_35_turbo_0125 = load_dataset(\"json\", data_files=gpt_35_turbo_0125_path, split=\"train\")\n",
    "print(f\"‚úÖ gpt_3.5_turbo_0125 dataset loaded: {len(dataset_gpt_35_turbo_0125)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5aa0c7-6801-4da2-b5b5-bca10d6adb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset_gpt_35_turbo_1106 = dataset_gpt_35_turbo_1106.train_test_split(test_size=0.1, seed=42)\n",
    "train_gpt_35_turbo_1106 = split_dataset_gpt_35_turbo_1106['train']\n",
    "test_gpt_35_turbo_1106 = split_dataset_gpt_35_turbo_1106['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77646cad-b2c5-44ff-9e7a-020f78541e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset_gpt_35_turbo_0125 = dataset_gpt_35_turbo_0125.train_test_split(test_size=0.1, seed=42)\n",
    "train_gpt_35_turbo_0125 = split_dataset_gpt_35_turbo_0125['train']\n",
    "test_gpt_35_turbo_0125 = split_dataset_gpt_35_turbo_0125['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56c51a",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "\n",
    "Load the Llama-3.2-1B-Instruct model and tokenizer for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73e0e26c-d9c8-49ff-b210-086ac1a83aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_gpt_35_turbo_1106\n",
    "test_db = test_gpt_35_turbo_1106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3197b6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using CUDA GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b738200423947d1bc3bb645e6691024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Formatted dataset: 3969 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24dd64142f3469ea65993dc62123f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Formatted dataset: 441 examples\n",
      "‚úÖ Model and tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "output_dir = \"output_model/gpt_35_turbo_1106_llama_model\"\n",
    "new_model_name = \"finetuned_llama_gpt_35_turbo_1106\"\n",
    "\n",
    "# Setup quantization for GPU\n",
    "bnb_config = None\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ Using CUDA GPU\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using CPU\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Format dataset with tokenizer\n",
    "formatted_train = dataset.map(formatting_prompts_func)\n",
    "print(f\"‚úÖ Formatted dataset: {len(formatted_train)} examples\")\n",
    "formatted_test = test_db.map(formatting_prompts_func)\n",
    "print(f\"‚úÖ Formatted dataset: {len(formatted_test)} examples\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61ace7",
   "metadata": {},
   "source": [
    "## 6. Configure and Apply LoRA\n",
    "\n",
    "Set up LoRA (Low-Rank Adaptation) configuration for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5f754f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68aeca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,409,024 || all params: 1,237,223,424 || trainable%: 0.1139\n",
      "‚úÖ LoRA applied successfully\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "LORA_RANK = 2\n",
    "lora_alpha = 40  # 2 * rank * 10\n",
    "lora_dropout = 0.05\n",
    "# lora_dropout = 0.\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    # target_modules=[\"down_proj\"],\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"‚úÖ LoRA applied successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48accc7a",
   "metadata": {},
   "source": [
    "## 7. Set Training Arguments\n",
    "\n",
    "Configure the training parameters optimized for Kaggle's GPU environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30383de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Training Config: 3 epochs, batch size 5, seq length 512\n",
      "‚úÖ Training configuration ready\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "# EPOCHS = 5\n",
    "EPOCHS = 3\n",
    "batch_size = 5 #20\n",
    "max_seq_length = 512\n",
    "learning_rate = 1e-4\n",
    "# output_dir = \"./llama-gpt_35_turbo_1106-test\"\n",
    "weight_decay = 0.5\n",
    "\n",
    "print(f\"üß™ Training Config: {EPOCHS} epochs, batch size {batch_size}, seq length {max_seq_length}\")\n",
    "\n",
    "# Create training configuration\n",
    "sft_config_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    logging_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=weight_decay,\n",
    "    save_steps=50,\n",
    "    max_steps=-1,\n",
    "    push_to_hub=False,\n",
    "    report_to=[],\n",
    "    run_name=f\"llama-gpt_35_turbo_1106-{EPOCHS}epoch\",\n",
    "    load_best_model_at_end=False,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    eval_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=0.1,\n",
    "    eval_on_start=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b8377",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n",
    "\n",
    "Create the trainer and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6f333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/LLM-fingerprints/.venv/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/workspace/LLM-fingerprints/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38eaab44f59e4339b44fa9ab8f7dfd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0944de52db48b4b82ee3d7cdf5f338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6d031fef5e489a93eb8008b3a41427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f3d24095b748168acba1721375b635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer created. Starting training on 3969 examples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 37/597 00:45 < 12:14, 0.76 it/s, Epoch 0.18/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.667339</td>\n",
       "      <td>0.957026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.656738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config_args,\n",
    "    train_dataset=formatted_train,\n",
    "    eval_dataset=formatted_test,\n",
    "    peft_config=lora_config,\n",
    "    # dataset_text_field=\"text\",\n",
    "    # max_seq_length=max_seq_length,\n",
    "    # packing=False,\n",
    ")\n",
    "print(f\"‚úÖ Trainer created. Starting training on {len(formatted_train)} examples...\")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"ÔøΩ Training completed! Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e67af",
   "metadata": {},
   "source": [
    "## 9. Save the Fine-Tuned Model\n",
    "\n",
    "Save the trained model and tokenizer for later use or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c941e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create archive for download\n",
    "import tarfile\n",
    "\n",
    "archive_name = f\"{new_model_name}.tar.gz\"\n",
    "with tarfile.open(archive_name, \"w:gz\") as tar:\n",
    "    tar.add(output_dir, arcname=new_model_name)\n",
    "\n",
    "# Cleanup\n",
    "if os.path.exists(\"temp_training_data.jsonl\"):\n",
    "    os.remove(\"temp_training_data.jsonl\")\n",
    "\n",
    "print(f\"‚úÖ Model archived as {archive_name} - ready for download!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c2f00-d6ed-4578-a739-09267f74d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718de07-80de-44db-9b6b-4b27d0e9fe27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
